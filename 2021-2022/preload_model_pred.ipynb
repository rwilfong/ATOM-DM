{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5dfea37-3f34-4fcd-bd50-d3164cf943e0",
   "metadata": {},
   "source": [
    "# Prediction Screening "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4a6f8-4417-4ab8-b0d1-e9ba2e295c5a",
   "metadata": {},
   "source": [
    "This notebook allows the user to load in their best performing machine learning model (under the variable best model). Then, using the load_from_model_file function from AMPL, the individual will load their model so it can be used to make predictions. From there, individuals can load SMILEs strings using the add_smiles function to read SMILEs strings into a dataframe and predict upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d763e1-8c7f-4e42-a3d4-85000bb2a4b5",
   "metadata": {},
   "source": [
    "##### In case you run into issues when running this notebook using SLURM\n",
    "-c for more cores, --mem for more memory, -t 1:00:00 for more time (can request up to 4 hours on Brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55239bdd-e0d8-4f48-af08-b5f58979d026",
   "metadata": {},
   "source": [
    "#### Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af18ff2b-09a6-425e-ad3f-0c9eca02f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:06:57.076067: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/brown/apps/proj/5.2.0-gcc-4.8.5-usib7od/lib:/apps/spack/brown/apps/geos/3.7.2-gcc-4.8.5-3vanyva/lib:/apps/spack/brown/apps/gdal/2.4.2-gcc-4.8.5-uj736h3/lib:/apps/spack/brown/apps/netcdf/4.5.0-gcc-6.3.0-d6fczmr/lib:/apps/spack/brown/apps/libtiff/4.0.10-gcc-6.3.0-6p5trqs/lib:/apps/spack/brown/apps/hdf5/1.8.16-gcc-6.3.0-7q7ndrz/lib:/apps/spack/brown/apps/hdf/4.2.14-gcc-6.3.0-2xg7pyg/lib:/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib:/apps/spack/brown/apps/r/4.0.0-gcc-6.3.0-hrvmcqp/rlib/R/lib:/apps/spack/brown/apps/openblas/0.3.7-gcc-6.3.0-qk24sho/lib:/apps/spack/brown/apps/tk/8.6.8-gcc-6.3.0-6qaesqb/lib:/apps/spack/brown/apps/tcl/8.6.8-gcc-6.3.0-n6mxabo/lib:/apps/spack/brown/apps/zlib/1.2.11-gcc-4.8.5-pkmj6e7/lib:/apps/spack/brown/apps/gcc/6.3.0-gcc-4.8.5-234aoxy/lib64:/apps/spack/brown/apps/gcc/6.3.0-gcc-4.8.5-234aoxy/lib:/apps/spack/brown/apps/mpc/1.1.0-gcc-4.8.5-eogmmao/lib:/apps/spack/brown/apps/mpfr/3.1.6-gcc-4.8.5-nsgsjfy/lib:/apps/spack/brown/apps/gmp/6.1.2-gcc-4.8.5-zn55wh7/lib:/scratch/brown/kamstut/tdm/opt/libffi-3.4.2/build/lib64:/usr/lib64\n",
      "2022-04-26 15:06:57.076125: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/bin/python -m pip install --upgrade pip' command.\n",
      "2022-04-26 15:08:48,603 Model tracker client not supported in your environment; will save models in filesystem only.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "import pandas as pd\n",
    "import os, json, sys, glob, pickle\n",
    "\n",
    "from atomsci.ddm.pipeline import model_pipeline as mp\n",
    "from atomsci.ddm.pipeline import parameter_parser as parse\n",
    "from atomsci.ddm.pipeline import perf_data\n",
    "\n",
    "from atomsci.ddm.pipeline import predict_from_model as pfm\n",
    "from atomsci.ddm.utils import curate_data\n",
    "from atomsci.ddm.utils import struct_utils as su\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689738a8-a944-43ad-a9db-7674de0268f4",
   "metadata": {},
   "source": [
    "#### Defining the load_model_from_file function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2791c27-045f-4d07-971d-87d96f2da8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile,tarfile,os\n",
    "def load_model_from_file(model_path) :\n",
    "    reload_dir = tempfile.mkdtemp()\n",
    "    model_fp = tarfile.open(model_path, mode='r:gz')\n",
    "    model_fp.extractall(path=reload_dir)\n",
    "    model_fp.close()\n",
    "    # Open the model_metadata.json file containing the reloaded model parameters\n",
    "    config_file_path = os.path.join(reload_dir, 'model_metadata.json')\n",
    "    with open(config_file_path) as f:\n",
    "        config = json.loads(f.read())\n",
    "    # Set the transformer_key parameter to point to the transformer pickle file we just extracted\n",
    "    try:\n",
    "        has_transformers = config['model_parameters']['transformers']\n",
    "        if has_transformers:\n",
    "            config['model_parameters']['transformer_key'] = \"%s/transformers.pkl\" % reload_dir\n",
    "    except KeyError:\n",
    "        pass\n",
    "    model_params = parse.wrapper(config)\n",
    "    model_params.result_dir = tempfile.mkdtemp()\n",
    "    model_params.featurizer = 'computed_descriptors'\n",
    "    split_uuid = model_params.split_uuid\n",
    "    dset_df = pd.read_csv(model_params.dataset_key)\n",
    "    directory = os.path.dirname(model_params.dataset_key)\n",
    "    dataset_name = os.path.splitext(os.path.basename(model_params.dataset_key))[0]\n",
    "    if model_params.split_strategy == 'k_fold_cv':\n",
    "        split_prefix = \"%d_fold_cv_%s\" % (model_params.num_folds, model_params.splitter)\n",
    "    else:\n",
    "        split_prefix = \"train_valid_test_%s\" % (model_params.splitter)\n",
    "    split_path = os.path.join(directory, '%s_%s_%s.csv' % (dataset_name, split_prefix, split_uuid))\n",
    "    split_df = pd.read_csv(split_path)\n",
    "\n",
    "    pipe = mp.create_prediction_pipeline_from_file(model_params, reload_dir)\n",
    "    return pipe,model_params,split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604bc186-10b8-4a91-a7b3-13079795b67f",
   "metadata": {},
   "source": [
    "### Reading in best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed995a4-591f-423f-80c1-32c7af34b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = '/depot/tdm-atom/data/shared/model_depot/team1/OPRD1_OPRK1_OPRM1_data_with_smiles_model_82d3a3b5-0627-4aa4-aca7-07e51181b12a.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f0da1-1174-4206-ae36-5b0c926afbbe",
   "metadata": {},
   "source": [
    "### Loading the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c220c550-6a55-49c3-9767-97c20238277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:08:49,506 ['ampl_version', 'time_generated', 'best_epoch', 'time_built', 'dataset_hash', 'dataset_metadata', 'training_metrics'] are not part of the accepted list of parameters and will be ignored\n",
      "2022-04-26 15:08:49,570 Created a dataset hash 'bb524d057c54ea478950d2a591bdb416' from dataset_key '/depot/tdm-atom/data/shared/model_depot/team1/OPRD1_OPRK1_OPRM1_data_with_smiles.csv'\n",
      "2022-04-26 15:08:49,715 ['ampl_version', 'time_generated', 'best_epoch', 'time_built', 'dataset_hash', 'dataset_metadata', 'training_metrics'] are not part of the accepted list of parameters and will be ignored\n",
      "2022-04-26 15:08:49,736 Created a dataset hash 'bb524d057c54ea478950d2a591bdb416' from dataset_key '/depot/tdm-atom/data/shared/model_depot/team1/OPRD1_OPRK1_OPRM1_data_with_smiles.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_model_tasks is deprecated and its value is ignored.\n",
      "num_model_tasks is deprecated and its value is ignored.\n",
      "Featurization = DynamicFeaturization with graphconv features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:08:49.801768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/spack/brown/apps/proj/5.2.0-gcc-4.8.5-usib7od/lib:/apps/spack/brown/apps/geos/3.7.2-gcc-4.8.5-3vanyva/lib:/apps/spack/brown/apps/gdal/2.4.2-gcc-4.8.5-uj736h3/lib:/apps/spack/brown/apps/netcdf/4.5.0-gcc-6.3.0-d6fczmr/lib:/apps/spack/brown/apps/libtiff/4.0.10-gcc-6.3.0-6p5trqs/lib:/apps/spack/brown/apps/hdf5/1.8.16-gcc-6.3.0-7q7ndrz/lib:/apps/spack/brown/apps/hdf/4.2.14-gcc-6.3.0-2xg7pyg/lib:/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib:/apps/spack/brown/apps/r/4.0.0-gcc-6.3.0-hrvmcqp/rlib/R/lib:/apps/spack/brown/apps/openblas/0.3.7-gcc-6.3.0-qk24sho/lib:/apps/spack/brown/apps/tk/8.6.8-gcc-6.3.0-6qaesqb/lib:/apps/spack/brown/apps/tcl/8.6.8-gcc-6.3.0-n6mxabo/lib:/apps/spack/brown/apps/zlib/1.2.11-gcc-4.8.5-pkmj6e7/lib:/apps/spack/brown/apps/gcc/6.3.0-gcc-4.8.5-234aoxy/lib64:/apps/spack/brown/apps/gcc/6.3.0-gcc-4.8.5-234aoxy/lib:/apps/spack/brown/apps/mpc/1.1.0-gcc-4.8.5-eogmmao/lib:/apps/spack/brown/apps/mpfr/3.1.6-gcc-4.8.5-nsgsjfy/lib:/apps/spack/brown/apps/gmp/6.1.2-gcc-4.8.5-zn55wh7/lib:/scratch/brown/kamstut/tdm/opt/libffi-3.4.2/build/lib64:/usr/lib64\n",
      "2022-04-26 15:08:49.835807: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-26 15:08:49.836377: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (brown-a025.rcac.purdue.edu): /proc/driver/nvidia/version does not exist\n",
      "2022-04-26 15:08:49.859468: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-26 15:08:49,908 This method is deprecated and will not be supported in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<atomsci.ddm.pipeline.model_pipeline.ModelPipeline at 0x2ab56a6d1640>,\n",
       " Namespace(bucket='public', dataset_key='/depot/tdm-atom/data/shared/model_depot/team1/OPRD1_OPRK1_OPRM1_data_with_smiles.csv', dataset_name=None, dataset_oid=None, datastore=False, id_col='compound_id', min_compound_number=200, response_cols=['target_OPRD1_standard_value', 'target_OPRK1_standard_value', 'target_OPRM1_standard_value'], save_results=False, smiles_col='base_rdkit_smiles', autoencoder_bucket=None, autoencoder_key=None, autoencoder_type='molvae', mol_vae_model_file=None, class_number=2, class_name=None, descriptor_bucket='public', descriptor_key=None, descriptor_oid=None, descriptor_spec_bucket='', descriptor_spec_key='/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/atomsci/ddm/data/descriptor_sets_sources_by_descr_type.csv', descriptor_type='moe', moe_threads=-1, ecfp_radius=2, ecfp_size=1024, featurizer='computed_descriptors', model_choice_score_type='r2', model_type='NN', prediction_type='regression', previously_featurized=True, uncertainty=False, verbose=False, optimizer_type='adam', mordred_cpus=None, baseline_epoch=30, batch_size=50, early_stopping_patience=30, early_stopping_min_improvement=0.0, bias_init_consts=None, dropouts=[0.25, 0.1, 0.1, 0.1, 0.1], layer_sizes=[128, 128, 64, 64, 32], learning_rate=0.001, max_epochs=200, weight_decay_penalty=0.0001, weight_decay_penalty_type='l2', weight_init_stddevs=None, is_ki=False, ki_convert_ratio=None, loss_func='poisson', rf_estimators=500, rf_max_depth=None, rf_max_features=32, base_splitter='scaffold', butina_cutoff=0.6, cutoff_date=None, date_col=None, num_folds=5, previously_split=True, split_strategy='train_valid_test', split_test_frac=0.1, split_uuid='94515da4-98b9-4f47-b3d2-0525133af0f0', split_valid_frac=0.1, splitter='index', feature_transform_type='normalization', response_transform_type='normalization', weight_transform_type=None, transformer_bucket='', transformer_key='/tmp/tmpme8p3y7t/transformers.pkl', transformer_oid='', transformers=True, umap_dim=10, umap_metric='euclidean', umap_min_dist=0.05, umap_neighbors=20, umap_targ_wt=0.0, xgb_colsample_bytree=1.0, xgb_gamma=0.0, xgb_learning_rate=0.1, xgb_max_depth=6, xgb_min_child_weight=1.0, xgb_n_estimators=100, xgb_subsample=1.0, collection_name='model_tracker', data_owner='gsk', data_owner_group='gsk_craa', model_bucket='public', model_dataset_oid=None, model_filter=None, model_uuid='82d3a3b5-0627-4aa4-aca7-07e51181b12a', output_dir=None, result_dir='/tmp/tmpvw1dt0xc', model_tarball_path=None, system='twintron-blue', config_file=None, num_model_tasks=3, dropout_list=None, hyperparam=False, hyperparam_uuid=None, layer_nums=None, lc_account='baasic', max_final_layer_size=32, max_jobs=80, node_nums=None, nn_size_scale_factor=1.0, python_path='/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/bin/python', rerun=True, script_dir='/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/atomsci/ddm', search_type='grid', split_only=False, shortlist_key=None, use_shortlist=False, slurm_account=None, slurm_export='ALL', slurm_nodes=1, slurm_options=None, slurm_partition='pbatch', slurm_time_limit=1440, lr=None, ls=None, ls_ratio=None, dp=None, rfe=None, rfd=None, rff=None, xgbg=None, xgbl=None, hp_checkpoint_save=None, hp_checkpoint_load=None, AttentiveFPModel_n_classes=None, AttentiveFPModel_graph_feat_size=None, AttentiveFPModel_self_loop=None, AttentiveFPModel_model=None, AttentiveFPModel_tensorboard=None, AttentiveFPModel_device=None, AttentiveFPModel_dropout=None, AttentiveFPModel_number_atom_features=None, AttentiveFPModel_batch_size=None, AttentiveFPModel_num_layers=None, AttentiveFPModel_output_types=None, AttentiveFPModel_optimizer=None, AttentiveFPModel_number_bond_features=None, AttentiveFPModel_loss=None, AttentiveFPModel_num_timesteps=None, AttentiveFPModel_log_frequency=None, AttentiveFPModel_wandb=None, GCNModel_n_classes=None, GCNModel_residual=None, GCNModel_self_loop=None, GCNModel_model=None, GCNModel_tensorboard=None, GCNModel_device=None, GCNModel_dropout=None, GCNModel_predictor_hidden_feats=None, GCNModel_activation=None, GCNModel_number_atom_features=None, GCNModel_batch_size=None, GCNModel_log_frequency=None, GCNModel_output_types=None, GCNModel_batchnorm=None, GCNModel_optimizer=None, GCNModel_loss=None, GCNModel_predictor_dropout=None, GCNModel_graph_conv_layers=None, GCNModel_wandb=None, MPNNModel_n_classes=None, MPNNModel_n_atom_feat=None, MPNNModel_n_pair_feat=None, MPNNModel_n_hidden=None, MPNNModel_T=None, MPNNModel_model=None, MPNNModel_tensorboard=None, MPNNModel_uncertainty=None, MPNNModel_dropout=None, MPNNModel_batch_size=None, MPNNModel_output_types=None, MPNNModel_optimizer=None, MPNNModel_loss=None, MPNNModel_log_frequency=None, MPNNModel_wandb=None, MPNNModel_M=None, GraphConvModel_n_classes=None, GraphConvModel_model=None, GraphConvModel_tensorboard=None, GraphConvModel_uncertainty=None, GraphConvModel_dropout=None, GraphConvModel_number_atom_features=None, GraphConvModel_batch_size=None, GraphConvModel_log_frequency=None, GraphConvModel_output_types=None, GraphConvModel_optimizer=None, GraphConvModel_batch_normalize=None, GraphConvModel_loss=None, GraphConvModel_dense_layer_size=None, GraphConvModel_graph_conv_layers=None, GraphConvModel_wandb=None, PytorchMPNNModel_n_classes=None, PytorchMPNNModel_node_out_feats=None, PytorchMPNNModel_self_loop=None, PytorchMPNNModel_model=None, PytorchMPNNModel_tensorboard=None, PytorchMPNNModel_device=None, PytorchMPNNModel_number_atom_features=None, PytorchMPNNModel_batch_size=None, PytorchMPNNModel_num_step_set2set=None, PytorchMPNNModel_num_layer_set2set=None, PytorchMPNNModel_log_frequency=None, PytorchMPNNModel_output_types=None, PytorchMPNNModel_optimizer=None, PytorchMPNNModel_num_step_message_passing=None, PytorchMPNNModel_number_bond_features=None, PytorchMPNNModel_loss=None, PytorchMPNNModel_edge_hidden_feats=None, PytorchMPNNModel_wandb=None, MolGraphConvFeaturizer_use_edges=None, MolGraphConvFeaturizer_use_partial_charge=None, MolGraphConvFeaturizer_use_chirality=None, WeaveFeaturizer_max_pair_distance=None, WeaveFeaturizer_graph_distance=None, WeaveFeaturizer_use_chirality=None, WeaveFeaturizer_explicit_H=None, ConvMolFeaturizer_atom_properties=None, ConvMolFeaturizer_use_chirality=None, ConvMolFeaturizer_per_atom_fragmentation=None, ConvMolFeaturizer_master_atom=None, dataset_hash='bb524d057c54ea478950d2a591bdb416'),\n",
       "                                                 cmpd_id subset  scaffold\n",
       " 0                     C=CCN(CC=C)CCc1c[nH]c2ccc(OC)cc12  valid      14.0\n",
       " 1     COc1ccc(NC(=O)N2CCN(c3ccc(-c4ccccc4)cc3)CC2)cc...  train      30.0\n",
       " 2     COc1ccccc1N1CCC(C(=O)Nc2ccc(OC)c(N3CCN(C)CC3)c...  train      30.0\n",
       " 3                                  NC1=NCc2cc(Cl)ccc2N1  train      30.0\n",
       " 4     N#Cc1ccc(CCOC(=O)NC2CCN(CCCC(c3ccc(F)cc3)c3ccc...  train      30.0\n",
       " ...                                                 ...    ...       ...\n",
       " 8869  C[C@@]1(c2cccc(N=S(C)(=O)O)c2)[C@H]2CN(CCCc3cc...  train      16.0\n",
       " 8870      NCCCCN(C[C@H]1Cc2ccccc2CN1)[C@H]1CCCc2cccnc21  train      40.0\n",
       " 8871  CC(=O)NC1(c2ccccn2)C[C@@H]2CC[C@H](C1)N2C(c1cc...  valid       1.0\n",
       " 8872  C[C@@]1(c2cccc(NS(C)(=O)=O)c2)[C@H]2CN(CCCc3cc...  train      16.0\n",
       " 8873  COc1ccc(C)c(OC(CCN2CCC3(CC2)C(=O)NCN3c2ccccc2)...  train      28.0\n",
       " \n",
       " [8874 rows x 3 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_from_file(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871eef7-ec0b-40c6-8b89-7b4f9e276c3b",
   "metadata": {},
   "source": [
    "### Reading in SMILEs Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f85bf-4b5f-4aae-b1a4-d963410f0edf",
   "metadata": {},
   "source": [
    "First, we are creating an empty list called smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf52873-84f3-4d78-bde3-7183eb521200",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00360a-27cc-48d7-8878-0afdbeea2770",
   "metadata": {},
   "source": [
    "Next, we are creating a function called add_smiles.\n",
    "\n",
    "This function will open each file it reads in the dataset directory (which you will define) and append the SMILEs strings to the smiles list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029c4e1a-9ab0-4200-bb45-cb467eefb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_smiles(dataset_directory):\n",
    "    for filename in os.scandir(dataset_directory):\n",
    "        with open(filename, \"r\") as ins:\n",
    "            for line in ins:\n",
    "                smiles.append(line.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19ba36-4b3c-4ec1-a443-3115c4ffea7b",
   "metadata": {},
   "source": [
    "Applying the add_smiles function\n",
    "\n",
    "The function will find all of the files in this directory and append the SMILEs strings. \n",
    "\n",
    "\n",
    "The defined directories are where the SMILEs strings are currently located in Brown for Purdue University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf29f70-6709-4de0-96ef-1dbe4b23ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the paths are commented out because the llnl kernel will die when trying to read all of them in (there's almost 1 billion SMILEs strings!) \n",
    "# So, you can do a couple and later just concatenate the csv files or dataframes (depending on which path you take).\n",
    "\n",
    "add_smiles(\"/depot/tdm-atom/data/allen99/S/H03\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H04\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H05\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H06\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H07\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H08\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H09\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H10\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H11\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H12\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H13\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H14\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H15\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H16\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H17\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H18\")\n",
    "#add_smiles(\"/depot/tdm-atom/data/allen99/S/H19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854c218-f34c-4a29-8366-c08786a52c04",
   "metadata": {},
   "source": [
    "Now, we are creating a dataframe from our list and looking at the shape and head of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028a21ea-36a6-4657-b494-4f83a6807e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CSC s_62____875850____876088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     smiles_col\n",
       "0  CSC s_62____875850____876088"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_df = pd.DataFrame(smiles, columns=['smiles_col'])\n",
    "print(smiles_df.shape)\n",
    "smiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1983b-f330-486c-9bf8-5fdeda05d83e",
   "metadata": {},
   "source": [
    "### Defining the predict_from_model_file function from AMPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ad8a73-1b78-4dd6-8d13-f18177a06847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_model_file(model_path, input_df, id_col='compound_id', smiles_col='smiles_col', response_col=None, is_featurized=False, dont_standardize=True):\n",
    "    input_df, pred_params = pfm._prepare_input_data(input_df, id_col, smiles_col, response_col, dont_standardize)\n",
    "\n",
    "    has_responses = ('response_cols' in pred_params)\n",
    "    pred_params = parse.wrapper(pred_params)\n",
    "\n",
    "    pipe = mp.create_prediction_pipeline_from_file(pred_params, reload_dir=None, model_path=model_path)\n",
    "    if pipe.params.model_type == 'xgboost':\n",
    "        pipe.params.uncertainty = False\n",
    "    pred_df = pipe.predict_full_dataset(input_df, contains_responses=has_responses, is_featurized=is_featurized,\n",
    "                                        dset_params=pred_params)\n",
    "    pred_df = pred_df.sort_values(by=id_col)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2ce2e-fa62-40d5-8ae8-b31b13a2fce4",
   "metadata": {},
   "source": [
    "#### Predicting on the SMILES Strings in the smiles_df using predict_from_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51576252-9570-454a-a008-eeabd60424c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_model_tasks is deprecated and its value is ignored.\n",
      "Featurization = DynamicFeaturization with graphconv features\n",
      "number of features: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:08:53.156877: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (3) does not match length of index (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_80467/4054434709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_pred = pfm.predict_from_model_file(model_path= best_model,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                          \u001b[0minput_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmiles_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                          \u001b[0mid_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'compound_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          \u001b[0msmiles_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'smiles_col'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0mresponse_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/atomsci/ddm/pipeline/predict_from_model.py\u001b[0m in \u001b[0;36mpredict_from_model_file\u001b[0;34m(model_path, input_df, id_col, smiles_col, response_col, conc_col, is_featurized, dont_standardize, AD_method, k, dist_metric, external_training_data)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexternal_training_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexternal_training_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     pred_df = pipe.predict_full_dataset(input_df, contains_responses=has_responses, is_featurized=is_featurized,\n\u001b[0m\u001b[1;32m    115\u001b[0m                                         dset_params=pred_params, AD_method=AD_method, k=k, dist_metric=dist_metric)\n\u001b[1;32m    116\u001b[0m     \u001b[0mpred_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/atomsci/ddm/pipeline/model_pipeline.py\u001b[0m in \u001b[0;36mpredict_full_dataset\u001b[0;34m(self, dset_df, is_featurized, contains_responses, dset_params, AD_method, k, dist_metric)\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'regression'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m                     \u001b[0mresult_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"%s_pred\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m                     \u001b[0mclass_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n",
      "\u001b[0;32m/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/brown/kamstut/tdm/apps/jupyter/kernels/llnl/.venv/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (3) does not match length of index (1)"
     ]
    }
   ],
   "source": [
    "model_pred = pfm.predict_from_model_file(model_path= best_model,\n",
    "                                         input_df=smiles_df, \n",
    "                                         id_col='compound_id', \n",
    "                                         smiles_col='smiles_col', \n",
    "                                         response_col=None, \n",
    "                                         dont_standardize=True, \n",
    "                                         is_featurized = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf52bb3-b890-4c72-9620-fa9b3a6a2898",
   "metadata": {},
   "source": [
    "### Looking at the returning dataframe from predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277a21b-245e-40f6-9d38-63b22baa352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f48ad0-4344-4e45-87eb-f6d9cb388e39",
   "metadata": {},
   "source": [
    "### Setting a Path and Saving the Dataframe as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82dab22f-3804-4296-8da4-0b75621c792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/rwilfong/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e52f5910-a919-4ec3-99be-55fd7b934288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred.to_csv(path+\"opioid.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llnl",
   "language": "python",
   "name": "tdm-brown-scratch-llnl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
